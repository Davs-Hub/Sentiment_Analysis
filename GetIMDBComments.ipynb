{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==4.4.3\n",
      "  Downloading selenium-4.4.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting urllib3~=1.26 (from urllib3[socks]~=1.26->selenium==4.4.3)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting trio~=0.17 (from selenium==4.4.3)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium==4.4.3)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from selenium==4.4.3) (2024.8.30)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium==4.4.3)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium==4.4.3)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from trio~=0.17->selenium==4.4.3) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium==4.4.3)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium==4.4.3)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium==4.4.3)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\linhkhanhspc\\documents\\th köln\\sem 3\\ki\\sentiment_analysis\\.venv\\lib\\site-packages (from trio~=0.17->selenium==4.4.3) (1.2.2)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium==4.4.3)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]~=1.26->selenium==4.4.3)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium==4.4.3)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.4.3)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.4.3-py3-none-any.whl (985 kB)\n",
      "   ---------------------------------------- 0.0/986.0 kB ? eta -:--:--\n",
      "   ------------------------------- ------- 786.4/986.0 kB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 986.0/986.0 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, urllib3, sniffio, PySocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed PySocks-1.7.1 attrs-24.2.0 cffi-1.17.1 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 selenium-4.4.3 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 urllib3-1.26.20 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium==4.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a dataframe to store the comments and their sentiment.\n",
    "- 0: negative\n",
    "- 1: positive\n",
    "- 2: neutral\n",
    "Since we are labeling the sentiment by hand, we want to get the rating in each comment to simplifying the process of categorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Scraping\n",
    " Now I want to scrape the IMDB website for movies and their reviews. First, I will scrape some imdb URLs to collect the movie ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_movie_id(list, url):\n",
    "    # URL for the IMDb top 250 movies\n",
    "    movie_id_list = list\n",
    "    url = url\n",
    "    # Set headers to simulate a browser visit\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Send an HTTP request to get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "\n",
    "    # Find the title section of each movie which contains the link to the movie\n",
    "    movies = soup.find_all(\"a\", class_ = \"ipc-title-link-wrapper\")\n",
    "    print(len(movies))\n",
    "    # Collect the id of each movie by extracting it from the link\n",
    "    \n",
    "    for link in movies:\n",
    "        href = link.get('href')  # Get the href attribute\n",
    "        movie_id = href.split('/')[2]\n",
    "        if not movie_id in movie_id_list:\n",
    "            movie_id_list.append(movie_id)\n",
    "\n",
    "    return movie_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "['tt0468569', 'tt1375666', 'tt0133093', 'tt0080684', 'tt6019206', 'tt0103064', 'tt0076759', 'tt0172495', 'tt15239678', 'tt1345836', 'tt0086190', 'tt1745960', 'tt0372784', 'tt1392190', 'tt1856101', 'tt0848228', 'tt0190332', 'tt1877830', 'tt5013056', 'tt3748528', 'tt2488496', 'tt0325710', 'tt9603212', 'tt4912910', 'tt0265086', 'tt0468547', 'tt15657202', 'tt0452668', 'tt2357423', 'tt26082308', 'tt4978420', 'tt3291150', 'tt1815862', 'tt1562871', 'tt5433138', 'tt0287978', 'tt23137904', 'tt8948208', 'tt14998742', 'tt9663764', 'tt14856980', 'tt8667828', 'tt6277462', 'tt5433140', 'tt2638144', 'tt1742334', 'tt13406136', 'tt0295701', 'tt3741700', 'tt1528100']\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    #best-rated action movies\n",
    "    \"https://www.imdb.com/search/title/?title_type=feature,tv_movie,tv_special,video&countries=US&interests=in0000002&sort=user_rating,desc\",\n",
    "\n",
    "    #worst-rated action movies\n",
    "    \"https://www.imdb.com/search/title/?title_type=feature,tv_movie,tv_special,video&countries=US&interests=in0000002&sort=user_rating,asc\",\n",
    "\n",
    "    ]\n",
    "#The chosen links are to ensure that the positive and negative comments are balanced\n",
    "movie_id_list = []\n",
    "for url in urls:\n",
    "    get_movie_id(movie_id_list, url)\n",
    "\n",
    "print(movie_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will visit each movie page with the retrieved ids and scrape the reviews and ratings.\n",
    "To make sure the amount of comments for each category is balanced, I will collect them based on rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_reviews(list,movie_id):\n",
    "    # URL for the IMDb user reviews page\n",
    "    urls = [#positive comments (rating 10)\n",
    "        f\"https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3&rating=10\", \n",
    "        #negative comments (rating 1)\n",
    "        f\"https://www.imdb.com/title/{movie_id}/reviews/?ref_=tt_ql_3&rating=1\",\n",
    "        #neutral comments (rating 5)\n",
    "        f\"https://www.imdb.com/title/{movie_id}/reviews/?ref_=tt_ql_3&rating=5\" ]\n",
    "\n",
    "    reviews_list = list\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "    }\n",
    "    for url in urls:\n",
    "        # Send an HTTP request to get the page content\n",
    "        response = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the review containers (limit to 5)\n",
    "        review_containers = soup.find_all('div', class_='ipc-list-card__content', limit = 5) \n",
    "\n",
    "        for container in review_containers:\n",
    "            review = container.find('div', class_='ipc-html-content-inner-div')\n",
    "            rating = container.find('span', class_='ipc-rating-star')\n",
    "            rating_score =  int(rating.text.split(\"/\")[0])\n",
    "            if review:\n",
    "                obj = {\"comment\": review.text,\n",
    "                    \"sentiment\": 1 if rating_score == 10 else 0 if rating_score ==1 else 2}\n",
    "                \n",
    "                reviews_list.append(obj)\n",
    "  \n",
    "    return reviews_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the text of each review\n",
    "reviews_list = []\n",
    "for movie_id in movie_id_list:\n",
    "    get_imdb_reviews(reviews_list,movie_id)\n",
    "\n",
    "print(len(reviews_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the data to the dataframe\n",
    "df = pd.DataFrame(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best movie ever. Heath ledger's work is phenom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a work of art. The finest sequel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is just what you want for the best movie. G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We've been subjected to enormous amounts of hy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let me start off by saying that I hated this f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment\n",
       "0  Best movie ever. Heath ledger's work is phenom...          1\n",
       "1  This movie is a work of art. The finest sequel...          1\n",
       "2  It is just what you want for the best movie. G...          1\n",
       "3  We've been subjected to enormous amounts of hy...          1\n",
       "4  Let me start off by saying that I hated this f...          2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 493 entries, 0 to 492\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   comment    493 non-null    object\n",
      " 1   sentiment  493 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1    196\n",
      "2    152\n",
      "0    145\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency of each unique value in the 'sentiment' column\n",
    "value_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    df.to_csv(\"movie_reviews.csv\", index=False)  # index=False if you don’t need row indices\n",
    "    print(\"File saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
