{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5r7lAI05MAL"
      },
      "outputs": [],
      "source": [
        "%pip install numpy tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MfvAlS6Y5MAX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIotqkq85MAZ"
      },
      "outputs": [],
      "source": [
        "%pip install nltk\n",
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6l9Bntm5MAb"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cH1tScXT5MAc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E-DFUurb5MAe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"movie_reviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sPIEzyw5MAf"
      },
      "source": [
        "# Vorverarbeitung der Texten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUvjJ8al5MAj",
        "outputId": "fddf234f-d428-45cd-ebe2-dccb7c355f6e"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.ToktokTokenizer()\n",
        "\n",
        "#Englische stopwords\n",
        "nltk.download('stopwords')\n",
        "stopword_list=stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rWkVHdxD5MAm"
      },
      "outputs": [],
      "source": [
        "#Funktion zur Tokenisierung des Textes und Entfernung von Stoppwörtern\n",
        "def tokenize(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LWp88ngS5MAo"
      },
      "outputs": [],
      "source": [
        "#Wende die Funktion auf die Spalte \"comment\" an\n",
        "df['comment']=df['comment'].apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljNCt4bu5MAq",
        "outputId": "4c0f8387-c1dd-4aa4-eb16-3617206474e8"
      },
      "outputs": [],
      "source": [
        "print(df.comment.loc[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnFPbuen5MAr",
        "outputId": "9e1c977c-7ec7-48db-df5d-2834943a71a9"
      },
      "outputs": [],
      "source": [
        "# Lemmatisierung\n",
        "# Download der erforderlichen Datensätze\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Für POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Iy1S9kRO5MAs"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# Zuordnung von POS (Part-of-Speech)-Tags von NLTK zu WordNet\n",
        "# Die Zuordnung von POS-Tags verbessert die Genauigkeit, indem sie sicherstellt, dass die Wörter korrekt lemmatisiert werden.\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatize(text):\n",
        "    pos_tags = pos_tag(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    new_text = ' '.join(lemmatized_words)\n",
        "    return new_text\n",
        "\n",
        "df['comment']=df['comment'].apply(lambda text: lemmatize(text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95RdC7Ou5MAt",
        "outputId": "8b282216-0521-4fbb-a110-b1a4c324a3c4"
      },
      "outputs": [],
      "source": [
        "print(df['comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J2aMWynQ5MAu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Entferne Sonderzeichen, Jahre\n",
        "def remove_things(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    # Remove years\n",
        "    text = re.sub(r\"\\b\\d{4}\\b\", \"\", text)\n",
        "\n",
        "    # Remove special characters\n",
        "    text=re.sub(r'[^a-zA-z0-9\\s]','',text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['comment']=df['comment'].apply(remove_things)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "aBF2VFT45MAv",
        "outputId": "1756f630-2fde-424b-b471-fa000069c2ab"
      },
      "outputs": [],
      "source": [
        "df.comment.loc[0:0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn7TZ9L05MAv"
      },
      "source": [
        "Term Frequency-Inverse Document Frequency model (TF-IDF)\n",
        "\n",
        "Es wird verwendet, um Textdokumente in eine Matrix von tf-idf-Merkmalen umzuwandeln."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n4-hLdDE5MAw"
      },
      "outputs": [],
      "source": [
        "#Tfidf vectorizer\n",
        "\n",
        "def tfidf_vectorize(text, tfidf = None):\n",
        "    if tfidf is None:\n",
        "        tfidf=TfidfVectorizer(min_df=0.0,max_df=1.0,use_idf=True,ngram_range=(1,3), max_features=100000)\n",
        "        tfidf.fit(df.comment)\n",
        "\n",
        "    new_text = tfidf.transform(text)\n",
        "    print(f\"Vocabulary size: {len(tfidf.get_feature_names_out())}\")\n",
        "\n",
        "    return new_text, tfidf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfLeXUb-5MAx",
        "outputId": "0667da88-ebbf-4292-ebd4-f4610ecab345"
      },
      "outputs": [],
      "source": [
        "text_count_matrix, fitted_tfidf = tfidf_vectorize(df.comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBBEOPmf5MAx",
        "outputId": "72ad685d-031b-4fab-97ae-973f08bc0d9d"
      },
      "outputs": [],
      "source": [
        "print(text_count_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pr98jnWC5MAy"
      },
      "outputs": [],
      "source": [
        "X =text_count_matrix\n",
        "y = df.sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4K80qIkX5MAy"
      },
      "outputs": [],
      "source": [
        "#Training- und Testdaten splitten\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IR7kun35MAz"
      },
      "source": [
        "## Neuronales Netz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "POc6Oh_w5MAz",
        "outputId": "442029c4-5c35-46f7-bad9-6a12da1da766"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xiEAJzwq5MA0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "R04UPeXn5MA1"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mJcoSpOz5MA1"
      },
      "outputs": [],
      "source": [
        "X_train_dense = X_train.toarray()\n",
        "#One hot encoding für die Label wird benutzt weil das geeignet für Multi-Label Klassifikation ist\n",
        "y_train_onehot = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0XgZKfhO5MA2"
      },
      "outputs": [],
      "source": [
        "X_test_dense = X_train.toarray()\n",
        "y_test_onehot = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IvZpKgsG9HKw"
      },
      "outputs": [],
      "source": [
        "#Training- und Validation-Daten splitten\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_dense, y_train_onehot, test_size=0.15, random_state=42, stratify=y_train_onehot\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "75VZJgF75MA3"
      },
      "outputs": [],
      "source": [
        "seq_model = keras.Sequential()\n",
        "#Input layer\n",
        "seq_model.add(Dense(128, activation='tanh', input_shape=(X_train_dense.shape[1],)))\n",
        "seq_model.add(Dropout(0.2))\n",
        "seq_model.add(Dense(64, activation='tanh'))\n",
        "seq_model.add(Dropout(0.3))\n",
        "seq_model.add(Dense(32, activation='tanh'))\n",
        "seq_model.add(Dropout(0.4))\n",
        "#Output layer\n",
        "seq_model.add(Dense(3, activation='softmax')) # <-- Aktivierungsfuktion für Multi-Label Klassifiation\n",
        "\n",
        "\n",
        "seq_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z2MN_GP5MA4",
        "outputId": "589ff17c-7dd4-45cd-872e-9980910bd40c"
      },
      "outputs": [],
      "source": [
        "# definiere batch size und epochs\n",
        "# training und validation loss darstellen\n",
        "\n",
        "history = seq_model.fit(X_train_final, y_train_final,\n",
        "validation_data = (X_val, y_val),\n",
        "epochs= 10,\n",
        "batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXclrbQU5MA4",
        "outputId": "620eeddf-48a9-4965-b120-19f929a6ef7e"
      },
      "outputs": [],
      "source": [
        "#Bewertung des Modells\n",
        "scores = seq_model.evaluate(X_test_dense, y_test_onehot, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ZmiKx-LH5MA5",
        "outputId": "e7866add-9ef8-49d5-a19f-79eaba7d7b1c"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UUZu38lw5MA5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DjYm3gw5MA6"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Fi_cdCKm5MA6",
        "outputId": "03fac25a-cedb-422f-f1aa-ade3912d7db3"
      },
      "outputs": [],
      "source": [
        "# 1: Vorhersagen generieren\n",
        "y_pred_prob = seq_model.predict(X_test_dense)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# 2:True Labels umwandeln (One-Hot kodiert)\n",
        "y_test_classes = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# 3: Confusion Matrix generieren\n",
        "cm = confusion_matrix(y_test_classes, y_pred)\n",
        "\n",
        "# 4: Visualisierung\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\", \"Neutral\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "v9CuX9S25MA6"
      },
      "outputs": [],
      "source": [
        "def run_NLP_pipeline(text, tfidf):\n",
        "    t1 = tokenize(text)\n",
        "    t2 = lemmatize(t1)\n",
        "    t3 = remove_things(t2)\n",
        "    t4, _ = tfidf_vectorize([t3], tfidf = tfidf)\n",
        "    t5 = t4.toarray()\n",
        "    return t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOT4PZts5MA6",
        "outputId": "f560936e-c5c6-4580-ca85-34a7c3f9075d"
      },
      "outputs": [],
      "source": [
        "# Du kannst hier deinen Kommentar schreiben\n",
        "sentence = [\"This is not good\"]\n",
        "sentence_1 = run_NLP_pipeline(sentence, fitted_tfidf)\n",
        "\n",
        "print(f\"Shape of input for model: {sentence_1.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ayUh78A5MA7",
        "outputId": "c61d8628-2f38-44d1-f6b0-c04ede2fdb60"
      },
      "outputs": [],
      "source": [
        "predictions = seq_model.predict(sentence_1)\n",
        "\n",
        "# Ermitteln der vorhergesagten Klasse (Index der maximalen Wahrscheinlichkeit)\n",
        "predicted_class = predictions.argmax(axis=-1)\n",
        "\n",
        "# Zuordnung des Index zur Klasse\n",
        "class_labels = [\"Negative\", \"Positive\", \"Neutral\"]  # 0, 1 , 2\n",
        "predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "# Fomarttieren\n",
        "formatted_predictions = {label: round(prob, 4) for label, prob in zip(class_labels, predictions[0])}\n",
        "print(f\"Probabilities: {formatted_predictions}\")\n",
        "\n",
        "print(f\"Predicted Class Index: {predicted_class[0]}\")\n",
        "print(f\"Predicted Label: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-kxDTaw5MA8"
      },
      "source": [
        "# Ein paar Visualisierungen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqbLUSIc5MBD"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeCFh-MA5MBE"
      },
      "outputs": [],
      "source": [
        "# Stoppwörter\n",
        "custom_stopwords = set(STOPWORDS)  # Get default stopwords\n",
        "custom_stopwords.update([\"movie\", \"film\", \"scene\", \"character\",\n",
        "                         \"plot\", \"make\", \"one\", \"actor\", \"see\",\n",
        "                         \"watch\", \"action\", \"drama\"])  # Add domain-specific words\n",
        "print(custom_stopwords)\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5, stopwords=custom_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdH-2-YL5MBE"
      },
      "outputs": [],
      "source": [
        "#word cloud für positive Wörter\n",
        "plt.figure(figsize=(10,10))\n",
        "positive_text=df[df.sentiment == 1].comment\n",
        "positive_string = \", \".join(positive_text)\n",
        "positive_words=WC.generate(positive_string)\n",
        "plt.imshow(positive_words,interpolation='bilinear')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAumFwMF5MBF"
      },
      "outputs": [],
      "source": [
        "#Word cloud für negative Wörter\n",
        "plt.figure(figsize=(10,10))\n",
        "negative_text=df[df.sentiment == 0].comment\n",
        "negative_string = \", \".join(negative_text)\n",
        "negative_words=WC.generate(negative_string)\n",
        "plt.imshow(negative_words,interpolation='bilinear')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Zt_KhH5MBF"
      },
      "outputs": [],
      "source": [
        "#Word cloud für neutrale Wörter\n",
        "plt.figure(figsize=(10,10))\n",
        "neutral_text=df[df.sentiment == 2].comment\n",
        "neutral_string = \", \".join(neutral_text)\n",
        "neutral_words=WC.generate(neutral_string)\n",
        "plt.imshow(neutral_words,interpolation='bilinear')\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
